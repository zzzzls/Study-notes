## CrawlSpider

`CrawlSpider` 是 Spider 的一个子类，最显著的功能就是 `LinkExtractors` 链接提取器，相较于 spider 来说，CrawlSpider 更适用于 **需要从内层网页爬取数据的场景**

> 例如：在新闻网站中，我们首先需要爬取新闻列表页，获取所有新闻的URL，然后再请求这些URL，获取新闻的详情数据。



## 执行流程

- 请求 `start_urls` 中的起始 url，并将响应结果发给**规则解析器**
- 规则解析器得到响应体后，交由**链接提取器**匹配 url
- 对匹配成功的 url 发起**请求**
- 根据规则解析器中的配置，决定该请求体的**去向**（转交函数处理 / 继续交由规则解析器进行匹配）



## 创建CrawlSpider

- `scrapy genspider -t crawl <爬虫名> <域名>`

    > 相较于默认指令，此处多了 `-t crawl` 参数，表示创建的爬虫文件基于 `CrawlSpider` 类，而不是基于 `Spider` 类

    

    ```python
    import scrapy
    # 导入CrawlSpider相关模块
    from scrapy.linkextractors import LinkExtractor
    from scrapy.spiders import CrawlSpider, Rule
    
    # 继承 CrawlSpider
    class PnewsSpider(CrawlSpider):
        name = 'pnews'  # 爬虫文件名
        start_urls = ['http://bj.people.com.cn/GB/233088/index1.html']
        rules = (
            # 规则解析器：将链接提取器提取到的url按照制定的规则发起请求
            Rule(
                # 链接提取器：在起始URL响应回来的页面中按指定规则提取URL
                LinkExtractor(allow=r'/n2/2021/\d+/c\d+-\d+\.html'),
                callback='parse_item'
            ),
        )
    
        # 内容解析方法
        def parse_item(self, response):
            title = response.xpath('//h1[@id="newstit"]/text()').get()
            print(title)
    ```



## 规则解析器

```python
Rule(
    link_extractor=None,
    callback=None,
    cb_kwargs=None,
    follow=None,
    process_links=None,
    process_request=None,
    errback=None
)
```

- link_extractor：链接提取器
- callback：回调函数，对链接提取器中的url发起请求，随后将请求体交由该函数
- cb_kwargs：向回调函数传递的参数
- follow：链接提取器提取出的 url 对应的响应是否继续被 rules 过滤（callback为None时，此参数为True）
- process_links：过滤链接提取器提取出来的url
- process_request：过滤  requests 请求

> `process_links` 和 `process_request` 函数的具体参数可进入源码查看

文档：[规则解析器](https://www.osgeo.cn/scrapy/topics/spiders.html?highlight=rules#scrapy.spiders.Rule)



## 链接提取器

```python
LinkExtractor(
    allow=(),
    deny=(),
    allow_domains=(),
    deny_domains=(),
    restrict_xpaths=(),
    tags=('a', 'area'),
    attrs=('href',),
    canonicalize=False,
    unique=True,
    process_value=None,
    deny_extensions=None,
    restrict_css=(),
    strip=True,
    restrict_text=None,
)
```

- allow：`str or list`，满足括号中正则表达式的 url 会被提取，默认空，全部匹配
- deny：`str or list`，满足括号中正则表达式的 url 不会被提取，优先级高于 allow
- allow_domains：会被提取链接的域名
- restrict_xpaths：通过 xpath 匹配 url

文档：[链接提取器](https://www.osgeo.cn/scrapy/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml)



## 注意事项

- 对于起始url，可以定义 `parse_start_url` 函数处理该 url 对应的响应
- 若页面链接为相对路径，链接提取器提取后会**自动补全**
- CrawlSpider 中不能重写 **parse** 方法，在父类中有特殊功能
- 如果多个 Rule 都满足一个 url，会从 rules中选择第一个满足的进行操作



















